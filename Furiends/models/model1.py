# -*- coding: utf-8 -*-
"""Model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFwX2qGOsblnRwUiYVu0SHyQHKEOugfI
"""

#!pip install dtreeviz.trees

# import libraries
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.autograd import Variable

from google.colab import drive
drive.mount('/content/drive')

# Import Data
data_name = "/content/drive/MyDrive/MLTeam/fake data - Sheet1.csv"
data = pd.read_csv(data_name)

data.head(10)

data.info()

# Data Cleaning process
cols = ["Have diseases", "Bmi (17-32)",	"Height(m)",	"weight(kg)",	"Age (20-80)",	"Gender", "Walk dog time (hr) (0-100)",	"Play time (hrs) (0-50)",	"Total steps",	"Calories", "Recovery"]
data = data[cols]

# Logistic Model
input = data.loc[:, 'Have diseases':"Recovery"]
target = data[["Recovery"]]

X, y = input, target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)

iterations_n = 1000
learning_rate = 0.01
num_feature = len(cols)

class PytorchLRModel(nn.Module):
  def __init__(self, num_feature):
    super(PytorchLRModel, self).__init__()
    self.fc1 = nn.Linear(num_feature, 2)
  
  def forward(self, x):
    x = F.sigmoid(self.fc1(x))
    return x[:,1]

clf = PytorchLRModel(num_feature)
criterion = nn.BCELoss()

import torch.optim as optim

optimizer = optim.SGD(clf.parameters(), lr=learning_rate)
input = Variable(torch.Tensor(X_train.values))
target = Variable(torch.Tensor(y_train.values)).long()

clf.train()
for iter_ in range(iterations_n):
  optimizer.zero_grad()
  scores = clf(input)
  loss = criterion(scores.unsqueeze(1), target.float())
  loss.backward()
  optimizer.step()

  if iter_ % 100 == 0:
    print('Iteration {}, Loss: {}'.format(iter_, loss.data.cpu().numpy()))

clf.eval()
target = Variable(torch.Tensor(y_test.values)).long()
y_hat = clf(Variable(torch.Tensor(X_test.values)))
predictions = (y_hat >= 0.5).float()
print(predictions)
print(target.shape)
accuracy = torch.eq(predictions, target).sum().item() / target.size()[0]
print("Accuracy: ", accuracy)

# Random Forest
rf = RandomForestRegressor(n_estimators = 500, max_features = 'sqrt', max_depth = 8, random_state = 18).fit(X_train, y_train)
prediction = rf.predict(X_test)
mse = mean_squared_error(y_test, prediction)
rmse = mse**.5
print(mse)
print(rmse)

# Neural Network
class NN():
  def __init__(self, input_n, layers=[256,128], output_n, lr, iterations):
    self.input_layer = input_n
    self.output_layer = output_n
    self.fc1 = nn.Linear(self.input_layer, layers[0])
    self.fc2 = nn.Linear(layers[0], layers[1])
    self.fc3 = nn.Linear(layers[1], self.output_layer)

    self.lr = lr
    self.iterations = iterations
  
  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = F.softmax(self.fc3(x), dim=1)
    return x

class NN_train_test():
  def __init__(self, input_n, layers, output_n, lrï¼Œ iterations, num_epochs):
    self.model = NN(input_n=input_n, layers=layers, output_n=output_n, lr=lr, iterations=iterations)
    self.criterion = nn.CrossEntropyLoss()
    self.optimizer = torch.optim.SGD(self.model.parameters(),lr=lr)
    self.num_epochs = num_epochs

  def train(X, y):
    for epoch in range(self.num_epochs):
      correct = 0
      l = 0
      for i in range(X.shape[0]):
        data = X.iloc[i, :]
        result = y.iloc[i, :]
        prediction = self.model(data)
        loss = self.criterion(prediction, result)
        loss.backward()
        optimizer.step()

        _, predictions = prediction.max(dim=1)
        correct += torch.eq(predictions, result).sum().item()
        l += result.shape[0]
    accuracy = correct / l
    print("Epoch: {}, Accuracy: {}".format(epoch, accuracy))

  def test(X, y):
    pass

# RNN