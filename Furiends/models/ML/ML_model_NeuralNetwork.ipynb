{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kZycVI0HWqUY"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from geopy.geocoders import Nominatim\n",
    "from translate import Translator\n",
    "import seaborn as sns\n",
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"C:\\\\Users\\\\Solar\\\\Furiends\\\\Furiends\\\\data\\\\raw\\\\Fake Data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bigT.sql',\n",
       " 'bigT2.csv',\n",
       " 'bigT3.csv',\n",
       " 'data_with_satisfication.csv',\n",
       " 'freq&dura per user per act_category.sql',\n",
       " 'frequency&duration_activity.csv',\n",
       " 'fulldb-09-03-2023-23-04-beta.sql',\n",
       " 'whole11.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View all the files in the data folder; \n",
    "os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>First_Name</th>\n",
       "      <th>Last_Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Rehabilitation</th>\n",
       "      <th>Pet_intimacy</th>\n",
       "      <th>category_name</th>\n",
       "      <th>Pet_Name</th>\n",
       "      <th>log_count</th>\n",
       "      <th>com_count</th>\n",
       "      <th>avg_loading</th>\n",
       "      <th>sum_duration</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Country</th>\n",
       "      <th>Country_abbrev</th>\n",
       "      <th>Continent</th>\n",
       "      <th>User_satisfication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Bruen</td>\n",
       "      <td>5272 Adela Overpass Suite 982\\r\\nRozellacheste...</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>'Maltese'</td>\n",
       "      <td>Barney</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.526948</td>\n",
       "      <td>0.268787</td>\n",
       "      <td>14374</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.516239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Eliseo</td>\n",
       "      <td>Trantow</td>\n",
       "      <td>2506 Mertz Inlet\\r\\nSouth Ezekielshire, OK 56215</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>'Golden Retriever'</td>\n",
       "      <td>Juliana</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>56215</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>AS</td>\n",
       "      <td>3.708759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oceane</td>\n",
       "      <td>Walter</td>\n",
       "      <td>9469 Frank Orchard Apt. 685\\r\\nLake Irmastad, ...</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>'Bulldog'</td>\n",
       "      <td>Sabrina</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.526618</td>\n",
       "      <td>0.225447</td>\n",
       "      <td>72714</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.004165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Zack</td>\n",
       "      <td>Marvin</td>\n",
       "      <td>982 Susan Manors\\r\\nPort Amiya, UT 97043-2723</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>'German Shepherd'</td>\n",
       "      <td>Yasmine</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550415</td>\n",
       "      <td>0.831412</td>\n",
       "      <td>97043</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>UA</td>\n",
       "      <td>EU</td>\n",
       "      <td>9.151314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Marco</td>\n",
       "      <td>Kessler</td>\n",
       "      <td>671 Troy Isle Suite 459\\r\\nNew Carriemouth, OK...</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>'German Shepherd'</td>\n",
       "      <td>Cristopher</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.821139</td>\n",
       "      <td>0.045328</td>\n",
       "      <td>61216</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>AS</td>\n",
       "      <td>4.684270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>596</td>\n",
       "      <td>Ethan</td>\n",
       "      <td>Schuppe</td>\n",
       "      <td>17224 Koelpin Rest\\r\\nTheresiashire, GA 69914</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>'Bulldog'</td>\n",
       "      <td>Filomena</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.396222</td>\n",
       "      <td>0.306561</td>\n",
       "      <td>69914</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BR</td>\n",
       "      <td>SA</td>\n",
       "      <td>5.773980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>597</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Legros</td>\n",
       "      <td>476 Veda View\\r\\nGroverfurt, AK 15766-2282</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>'Shi-ba-inu'</td>\n",
       "      <td>Herta</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.425255</td>\n",
       "      <td>0.201590</td>\n",
       "      <td>15766</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BR</td>\n",
       "      <td>SA</td>\n",
       "      <td>3.474042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>598</td>\n",
       "      <td>Ilene</td>\n",
       "      <td>Dietrich</td>\n",
       "      <td>934 Josianne Inlet\\r\\nLavonnechester, AZ 30057...</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>'Poodle'</td>\n",
       "      <td>Lamont</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.215269</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>30057</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>TW</td>\n",
       "      <td>AS</td>\n",
       "      <td>1.505491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>599</td>\n",
       "      <td>Novella</td>\n",
       "      <td>Becker</td>\n",
       "      <td>76281 Germaine Valley\\r\\nBoscobury, DE 99069-3814</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>'Golden Retriever'</td>\n",
       "      <td>Wendy</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.427905</td>\n",
       "      <td>0.757455</td>\n",
       "      <td>99069</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.215302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>600</td>\n",
       "      <td>Rodrick</td>\n",
       "      <td>Pacocha</td>\n",
       "      <td>840 Pfeffer Club\\r\\nEast Amiya, MO 28324</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>'Maltese'</td>\n",
       "      <td>Alia</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.378318</td>\n",
       "      <td>0.289861</td>\n",
       "      <td>28324</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>UA</td>\n",
       "      <td>EU</td>\n",
       "      <td>3.795337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_id First_Name Last_Name  \\\n",
       "0          1       Jane     Bruen   \n",
       "1          2     Eliseo   Trantow   \n",
       "2          3     Oceane    Walter   \n",
       "3          4       Zack    Marvin   \n",
       "4          5      Marco   Kessler   \n",
       "..       ...        ...       ...   \n",
       "570      596      Ethan   Schuppe   \n",
       "571      597     Austin    Legros   \n",
       "572      598      Ilene  Dietrich   \n",
       "573      599    Novella    Becker   \n",
       "574      600    Rodrick   Pacocha   \n",
       "\n",
       "                                               Address  Age  Gender  \\\n",
       "0    5272 Adela Overpass Suite 982\\r\\nRozellacheste...   28       1   \n",
       "1     2506 Mertz Inlet\\r\\nSouth Ezekielshire, OK 56215   12       1   \n",
       "2    9469 Frank Orchard Apt. 685\\r\\nLake Irmastad, ...   64       1   \n",
       "3        982 Susan Manors\\r\\nPort Amiya, UT 97043-2723   38       1   \n",
       "4    671 Troy Isle Suite 459\\r\\nNew Carriemouth, OK...   28       1   \n",
       "..                                                 ...  ...     ...   \n",
       "570      17224 Koelpin Rest\\r\\nTheresiashire, GA 69914   79       2   \n",
       "571         476 Veda View\\r\\nGroverfurt, AK 15766-2282   80       1   \n",
       "572  934 Josianne Inlet\\r\\nLavonnechester, AZ 30057...   74       1   \n",
       "573  76281 Germaine Valley\\r\\nBoscobury, DE 99069-3814   29       2   \n",
       "574           840 Pfeffer Club\\r\\nEast Amiya, MO 28324   25       1   \n",
       "\n",
       "     Rehabilitation  Pet_intimacy       category_name    Pet_Name  log_count  \\\n",
       "0                 2      0.414141           'Maltese'      Barney   0.333333   \n",
       "1                 1      0.111111  'Golden Retriever'     Juliana   0.000000   \n",
       "2                 2      0.909091           'Bulldog'     Sabrina   0.500000   \n",
       "3                 1      0.858586   'German Shepherd'     Yasmine   0.666667   \n",
       "4                 1      0.181818   'German Shepherd'  Cristopher   0.250000   \n",
       "..              ...           ...                 ...         ...        ...   \n",
       "570               1      0.818182           'Bulldog'    Filomena   0.250000   \n",
       "571               2      0.909091        'Shi-ba-inu'       Herta   0.166667   \n",
       "572               2      0.272727            'Poodle'      Lamont   0.083333   \n",
       "573               2      0.090909  'Golden Retriever'       Wendy   0.500000   \n",
       "574               2      0.757576           'Maltese'        Alia   0.333333   \n",
       "\n",
       "     com_count  avg_loading  sum_duration  Zipcode         Country  \\\n",
       "0     0.333333     0.526948      0.268787    14374   United States   \n",
       "1     0.083333     1.000000      0.003579    56215     South Korea   \n",
       "2     0.166667     0.526618      0.225447    72714   United States   \n",
       "3     0.583333     0.550415      0.831412    97043         Ukraine   \n",
       "4     0.500000     0.821139      0.045328    61216     South Korea   \n",
       "..         ...          ...           ...      ...             ...   \n",
       "570   0.333333     0.396222      0.306561    69914          Brazil   \n",
       "571   0.500000     0.425255      0.201590    15766          Brazil   \n",
       "572   0.333333     0.215269      0.004374    30057          Taiwan   \n",
       "573   0.166667     0.427905      0.757455    99069   United States   \n",
       "574   0.333333     0.378318      0.289861    28324         Ukraine   \n",
       "\n",
       "    Country_abbrev Continent  User_satisfication  \n",
       "0               US       NaN            3.516239  \n",
       "1               KR        AS            3.708759  \n",
       "2               US       NaN            4.004165  \n",
       "3               UA        EU            9.151314  \n",
       "4               KR        AS            4.684270  \n",
       "..             ...       ...                 ...  \n",
       "570             BR        SA            5.773980  \n",
       "571             BR        SA            3.474042  \n",
       "572             TW        AS            1.505491  \n",
       "573             US       NaN            5.215302  \n",
       "574             UA        EU            3.795337  \n",
       "\n",
       "[575 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(root_path, 'data_with_satisfication.csv')).drop([\"Unnamed: 0\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    575.000000\n",
       "mean       4.772740\n",
       "std        1.406016\n",
       "min        1.296136\n",
       "25%        3.776412\n",
       "50%        4.841757\n",
       "75%        5.711057\n",
       "max        9.151314\n",
       "Name: User_satisfication, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"User_satisfication\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x239474bb820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFwCAYAAACGt6HXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAehElEQVR4nO3de7xcZX3v8c8XgoJcBCRwQkgEK6VeAY3WQrXaoAevUKtcKho9eOB4WivV2qL1eLA9pwettdrWWiJYolDKRRGsFqEIXooiAQFBsCgiCYlJQK6iQuB3/lhr6ybsZO/s7NnPJPm8X695zaxnZq31m9mzv/PMM2ueSVUhSZp+W7QuQJI2VwawJDViAEtSIwawJDViAEtSIwawJDViAGvaJLk+yQsncLvfSbIkyX1J9p/oepOo59+SLJjq7U5w369LcuEUbu8Xj1GSE5KcNoXbfneSk6dqe/qleBzwcEtSwN5V9b1RbScAT66qowa87z2AjwC/BWwF3Ar8dVWdOoF1TwWWVtV7JrHf7wNvr6rz1nfddWzzBKbhMev3dSrwe8DP+6YfAp8DTqyquyexrfV6HDfkvvYhflpV7bG+62r92QMWAElmjNH8KWAJ8ETgCcAbgBXTUM4TgeunYT+D9IGq2h6YCbwJeB7wH0m2ncqdrOXvpo1FVXka4hNQdL2Z0W0n0PVSAHYB/hW4C/gx8FVgi/663YFPA6uAHwB/uMY2zgFOA+4B3jzGvu8D9ltHbWcDPwLuBr4CPK1vPwZ4EHig38bn+vZbgIP6y88FFvf7XgF8CHhsf/sCfgJ8f4z1tgTeDXwfuBe4EpjTX/cRuheMe/r25/ftB/e1PNhv/5q+/dKR+03XGXkPXW91JfBJ4PH9dXv2NS2gexdwO/Bn63hcTgX+zxpt2wPLgT/ol98IfK2/HOBv+v3eDVwLPH2cx/FP+9v9HJixxmM08rc9s3+MrgL2XdtzaqReYFvgp8DD/f7uo3sOnUD/fOtv/yq6F8i7+sfwKaOuuwX44762u/satm79fzSsJ3vAG793AEvpelq70YVTJdmC7m3vNcBsYD5wXJL/OmrdQ+j+UXcETh9j298APprkiCRzx7j+34C9gV3p/slPB6iqhf3lD1TVdlX1yjHW/QjwkaraAfgV4Kyq+nlVbddfv29V/coY670dOBJ4GbAD8N+A+/vrrgD2A3YG/hk4O8nWVXUB8JfAmX09+46x3Tf2pxcBTwK2A/5+jdv8JrAP3WP53iRPGWM7Y6qqe4GLgOePcfVLgBcAv0r3tzgcuGOcx/FI4OXAjlW1eoxtHkL3AjnyWHw2yVbj1PgT4KXAsn5/21XVstG3SfKrwBnAcXTPuS8An0vymFE3O4zuRW8v4Jl0j6vGYABv/B4EZgFPrKoHq+qr1XVFngPMrKo/r6oHqupm4OPAEaPW/XpVfbaqHq6qn46x7dfS9aj/F/CDJFcnec7IlVX1iaq6t6p+TtdL2jfJ49ej7icn2aWq7quqb0xwvTcD76mq71bnmqq6o6/ntKq6o6pWV9Vf0/Wo95ngdl8HfKiqbq6q+4B3AUes8Rb/fVX106q6hu6FbawgX5dldIG4pgfpesi/Rve5zA1VtXycbf1tVS1Zy98N4MqqOqeqHqR7d7E13TDIhjoc+HxVXdRv+4PANsABa9S2rKp+TNcJ2G8K9rtJMoCH30N0H4CNthXdPy3AXwHfAy5McnOS4/v2JwK7J7lr5ETXO95t1HaWrGvHVXVnVR1fVU/r17uarieVJFsmOTHJ95PcQ/fWE7ohkYk4mq7Hd2OSK5K8YoLrzaEbfniUJO9IckOSu/v7+/j1qGd3uuGHET+ke2s/+vH60ajL99P1ktfHbLphokeoqi/R9bY/CqxIsjDJDuNsa51/u9HXV9XDdO+Sdl+/csf0iMep3/YSuvs2YkMfp82GATz8bqUbgxxtL/p/gr4H+o6qehLwSuDtSebT/VP8oKp2HHXavqpeNmo7Ez4Epqpup+vt7E7Xi/s9ure5B9EF3UiNmci2q+qmqjqSbvji/cA5E/yAagndkMUjJHk+3bjoYcBOVbUj3RjkhOqh650+cdTyXGA1U/ShY5Lt6B6rr451fVX9bVU9G3ga3QvTO0euWssmx7s/c0btewtgD7r7CF0oPm7Ubf/Lemz3EY9TkvT7um2c9TQGA3j4nQm8J8keSbZIchBd0J4DkOQVSZ7c/yPcQ9djfgj4JnBPkj9Nsk3fY3366CGE8SR5f7/OjCTbA28Bvte/5d+e7gOgO+j+mf9yjdVX0I2lrm3bRyWZ2feg7uqbH5pAWScDf5Fk774n/swkT+jrWU33geOMJO+lGyMeXc+efRiN5Qzgj5Ls1YflyJjxWOOrE5bksUmeDXwWuBP4pzFu85wkv96P0f4E+Bm/fCzW+Tiuw7OTvLofQjmO7m81MsxzNfB7/XPiYLrDDEesAJ6wjqGks4CXJ5nf1/uOftuXTaLGzZ4BPPz+nO7J/TW6f+APAK+rquv66/cG/p3uE+uvA/9QVZdW1UN0Qb0f3REQt9OF10THaKEL1nPpAvJmup7Pq/rrPknXC78N+A6//OcecQrw1H7447NjbPtg4Pok99F9IHdEVf1sAjV9iC4ELqR7wTmFbgzyi3QfCv5nX9fPeOTb9LP78zuSXDXGdj9Bd9jdV+ger58Bb51APWvzJ0nupRty+CTdURkH9B90rWkHuvH5O/va76B7twHjP45rcx7deO2dwOuBV/djtgBvo3tu3EU39v2L7VbVjXQvRjf3+3zEsEVVfRc4Cvg7uufUK4FXVtUD61Gben4RQ5IasQcsSY0YwJLUiAEsSY0YwJLUyEYxkcfBBx9cF1xwQesyJGmyMlbjRtEDvv3221uXIElTbqMIYEnaFBnAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAGnqz58wlyaRPs+fMbX0XpDFtFPMBa/O2bOkSDj9p8r96fuaxB0xhNdLUsQcsSY0YwJLUiAEsjWNDx6Adh9baOAYsjWNDx6DBcWiNzR6wJDViAEtSIwawJDUysABOsk+Sq0ed7klyXJKdk1yU5Kb+fKdB1SBJw2xgAVxV362q/apqP+DZwP3AucDxwMVVtTdwcb8sSZud6RqCmA98v6p+CBwCLOrbFwGHTlMNkjRUpiuAjwDO6C/vVlXLAfrzXcdaIckxSRYnWbxq1appKlOSps/AAzjJY4BXAWevz3pVtbCq5lXVvJkzZw6mOElqaDp6wC8FrqqqFf3yiiSzAPrzldNQgyQNnekI4CP55fADwPnAgv7yAuC8aahBkobOQAM4yeOAFwOfGdV8IvDiJDf11504yBokaVgNdC6IqrofeMIabXfQHRUhSZs1vwknSY0YwJLUiNNRatO3xQyStK5CehQDWJu+h1f7m3IaSg5BSFIjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAa51mz5lLkg06zZ4zt/XdkIaSxwFrnZYtXbJBx9CCx9FKa2MPWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaGWgAJ9kxyTlJbkxyQ5LfSLJzkouS3NSf7zTIGiRpWA26B/wR4IKq+jVgX+AG4Hjg4qraG7i4X5akzc7AAjjJDsALgFMAquqBqroLOARY1N9sEXDooGqQpGE2yB7wk4BVwD8l+VaSk5NsC+xWVcsB+vNdx1o5yTFJFidZvGrVqgGWKUltDDKAZwDPAj5WVfsDP2E9hhuqamFVzauqeTNnzhxUjZLUzCADeCmwtKou75fPoQvkFUlmAfTnKwdYgyQNrYEFcFX9CFiSZJ++aT7wHeB8YEHftgA4b1A1SNIwmzHg7b8VOD3JY4CbgTfRhf5ZSY4GbgVeO+AaJGkoDTSAq+pqYN4YV80f5H4laWPgN+EkqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIaMYAlqREDWJIamdG6AG0GtphBktZVSEPHANbgPbyaw0+6bNKrn3nsAVNYjDQ8HIKQpEYMYElqxACWpEYGOgac5BbgXuAhYHVVzUuyM3AmsCdwC3BYVd05yDokaRhNRw/4RVW1X1XN65ePBy6uqr2Bi/tlSdrstBiCOARY1F9eBBzaoAZJam7QAVzAhUmuTHJM37ZbVS0H6M93HWvFJMckWZxk8apVqwZcpjRg/bHQkz3NnjO39T3QAAz6OOADq2pZkl2Bi5LcONEVq2ohsBBg3rx5NagCpWnhsdAaw0B7wFW1rD9fCZwLPBdYkWQWQH++cpA1SNKwGlgAJ9k2yfYjl4GXANcB5wML+pstAM4bVA2SNMwGOQSxG3BuPwfADOCfq+qCJFcAZyU5GrgVeO0Aa5CkoTWwAK6qm4F9x2i/A5g/qP1K0sbCb8JJUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1MqEATnLgRNokSRM30R7w302wTZI0QTPWdWWS3wAOAGYmefuoq3YAthxkYerMnjOXZUuXTHr93feYw21Lbp3CiiRNlXUGMPAYYLv+dtuPar8HeM1EdpBkS2AxcFtVvSLJzsCZwJ7ALcBhVXXn+pW9+Vi2dAmHn3TZpNc/89gDprAaSVNpnQFcVV8Gvpzk1Kr64ST38TbgBrpeM8DxwMVVdWKS4/vlP53ktiVpozXRMeDHJlmY5MIkXxo5jbdSkj2AlwMnj2o+BFjUX14EHLo+BUvSpmK8IYgRZwP/SBekD63H9j8M/AmPHL7YraqWA1TV8iS7jrVikmOAYwDmzp27HrvUI2wxgyStq5A0hokG8Oqq+tj6bDjJK4CVVXVlkheub2FVtRBYCDBv3rxa3/XVe3i1Y8jSkJpoAH8uyf8EzgV+PtJYVT9exzoHAq9K8jJga2CHJKcBK5LM6nu/s4CVk6xdkjZqEx0DXgC8E7gMuLI/LV7XClX1rqrao6r2BI4AvlRVRwHn99sb2e55k6hbkjZ6E+oBV9VeU7jPE4GzkhwN3Aq8dgq3LUkbjQkFcJI3jNVeVZ+cyPpVdSlwaX/5DmD+xMqTpE3XRMeAnzPq8tZ0AXoVMKEAliQ92kSHIN46ejnJ44FPDaQiSdpMTHY6yvuBvaeyEEna3Ex0DPhzwMixuFsCTwHOGlRRkrQ5mOgY8AdHXV4N/LCqlg6gHknabExoCKKflOdGuq8U7wQ8MMiiJGlzMNFfxDgM+CbdMbuHAZcnmdB0lJKksU10COLPgOdU1UqAJDOBfwfOGVRhkrSpm+hREFuMhG/vjvVYV5I0hon2gC9I8kXgjH75cOALgylJkjYP4/0m3JPp5u99Z5JXA78JBPg6cPo01CdJm6zxhhE+DNwLUFWfqaq3V9Uf0fV+PzzY0iRp0zZeAO9ZVdeu2VhVi+l+VFOSNEnjBfDW67hum6ksRJI2N+MF8BVJ/vuajf1cvlcOpiRJ2jyMdxTEccC5SV7HLwN3HvAY4HcGWJckbfLWGcBVtQI4IMmLgKf3zZ+vqnF/kl6StG4TnQ/4EuCSAdciSZsVv80mSY0YwJLUiAEsSY0YwJLUiAEsbQy2mEGSSZ9mz5nb+h5oDBOdDU1SSw+v5vCTLpv06mcee8AUFqOpYg9YkhoxgCWpEQNYkhoxgCWpEQNYkhoxgCWpEQNYkhoZWAAn2TrJN5Nck+T6JO/r23dOclGSm/rznQZVgyQNs0H2gH8O/HZV7QvsBxyc5HnA8cDFVbU3cHG/LEmbnYEFcHXu6xe36k8FHAIs6tsXAYcOqgZJGmYDHQNOsmWSq4GVwEVVdTmwW1UtB+jPd13LusckWZxk8apVqwZZpiQ1MdAArqqHqmo/YA/guUmePs4qo9ddWFXzqmrezJkzB1ajJLUyLUdBVNVdwKXAwcCKJLMA+vOV01GDJA2bQR4FMTPJjv3lbYCDgBuB84EF/c0WAOcNqgZJGmaDnI5yFrAoyZZ0QX9WVf1rkq8DZyU5GrgVeO0Aa5CkoTWwAK6qa4H9x2i/A5g/qP1K0sbCb8JJUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgAP0Ow5c0myQSdJm65BTsi+2Vu2dAmHn3TZBm3jzGMPmKJqJA0be8CS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1MjAAjjJnCSXJLkhyfVJ3ta375zkoiQ39ec7DaoGSRpmg+wBrwbeUVVPAZ4H/H6SpwLHAxdX1d7Axf2yJG12BhbAVbW8qq7qL98L3ADMBg4BFvU3WwQcOqgaJGmYTcsYcJI9gf2By4Hdqmo5dCEN7LqWdY5JsjjJ4lWrVk1HmZI0rQYewEm2Az4NHFdV90x0vapaWFXzqmrezJkzB1egJDUy0ABOshVd+J5eVZ/pm1ckmdVfPwtYOcgaJGlYDfIoiACnADdU1YdGXXU+sKC/vAA4b1A1SNIwG+TP0h8IvB74dpKr+7Z3AycCZyU5GrgVeO0Aa5CkoTWwAK6qrwFZy9XzB7VfSdpY+E04SWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgaXOwxQySbNBp9py5re/FJmeQk/FIGhYPr+bwky7boE2ceewBU1SMRtgDlqRGDGBJasQAXofZc+Zu0JiZJK2LY8DrsGzpkg0aN3PMTNK62AOWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWNDEbOKew8wk/mnNBSJqYDZxT2LlRHs0esCQ1YgBLUiMGsCQ1MrAATvKJJCuTXDeqbeckFyW5qT/faVD7l6RhN8ge8KnAwWu0HQ9cXFV7Axf3y5K0WRpYAFfVV4Afr9F8CLCov7wIOHRQ+5ekYTfdY8C7VdVygP5817XdMMkxSRYnWbxq1appK1CSpsvQfghXVQural5VzZs5c2brciRpyk13AK9IMgugP185zfuXpKEx3QF8PrCgv7wAOG+a9y9JQ2OQh6GdAXwd2CfJ0iRHAycCL05yE/DiflmSNksDmwuiqo5cy1XzB7VPSdqYDO2HcJK0qTOAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1jS9NhiBkkmfZo9Z27rezDlBjYXhCQ9wsOrOfykyya9+pnHHjCFxQwHe8CS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNbNIBPHvO3A367rmkTceG5sEg5qPYpOeCWLZ0id89lwRseB7A1GfCJt0DlqRhZgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ10iSAkxyc5LtJvpfk+BY1SFJr0x7ASbYEPgq8FHgqcGSSp053HZLUWose8HOB71XVzVX1APAvwCEN6pCkplJV07vD5DXAwVX15n759cCvV9UfrHG7Y4Bj+sV9gO/2l3cBbp+mcidimOqxlrENUy0wXPVYy9imupbbq+rgNRtbzAUx1iw3j3oVqKqFwMJHrZwsrqp5gyhsMoapHmsZ2zDVAsNVj7WMbbpqaTEEsRSYM2p5D2BZgzokqakWAXwFsHeSvZI8BjgCOL9BHZLU1LQPQVTV6iR/AHwR2BL4RFVdvx6beNSwRGPDVI+1jG2YaoHhqsdaxjYttUz7h3CSpI7fhJOkRgxgSWpkowngJJ9IsjLJdUNQy5wklyS5Icn1Sd7WsJatk3wzyTV9Le9rVcuomrZM8q0k/zoEtdyS5NtJrk6yuHEtOyY5J8mN/XPnNxrWsk//mIyc7klyXMN6/qh//l6X5IwkWzes5W19HdcP+jHZaMaAk7wAuA/4ZFU9vXEts4BZVXVVku2BK4FDq+o7DWoJsG1V3ZdkK+BrwNuq6hvTXcuomt4OzAN2qKpXtKqjr+UWYF5VNT/AP8ki4KtVdXJ/BNDjququxmWNTA9wG90Xon7YYP+z6Z63T62qnyY5C/hCVZ3aoJan030797nAA8AFwFuq6qZB7G+j6QFX1VeAH7euA6CqllfVVf3le4EbgNmNaqmquq9f3Ko/NXtVTbIH8HLg5FY1DKMkOwAvAE4BqKoHhiF8e/OB77cI31FmANskmQE8jnbfDXgK8I2qur+qVgNfBn5nUDvbaAJ4WCXZE9gfuLxhDVsmuRpYCVxUVc1qAT4M/AnwcMMaRivgwiRX9l9vb+VJwCrgn/rhmZOTbNuwntGOAM5otfOqug34IHArsBy4u6oubFTOdcALkjwhyeOAl/HIL45NKQN4AyTZDvg0cFxV3dOqjqp6qKr2o/tW4XP7t1HTLskrgJVVdWWL/a/FgVX1LLrZ936/H8pqYQbwLOBjVbU/8BOg+VSs/VDIq4CzG9awE92EXHsBuwPbJjmqRS1VdQPwfuAiuuGHa4DVg9qfATxJ/Xjrp4HTq+ozresB6N/SXgo8atKPaXIg8Kp+3PVfgN9OclqjWgCoqmX9+UrgXLqxvRaWAktHvTs5hy6QW3spcFVVrWhYw0HAD6pqVVU9CHwGOKBVMVV1SlU9q6peQDfsOZDxXzCAJ6X/4OsU4Iaq+lDjWmYm2bG/vA3dk/nGFrVU1buqao+q2pPube2XqqpJTwYgybb9h6T0b/dfQvcWc9pV1Y+AJUn26ZvmA9P+oe0YjqTh8EPvVuB5SR7X/2/Np/tcpYkku/bnc4FXM8DHp8VsaJOS5AzghcAuSZYC/7uqTmlUzoHA64Fv92OvAO+uqi80qGUWsKj/JHsL4Kyqan7415DYDTi3+59mBvDPVXVBw3reCpzev+2/GXhTw1roxzhfDBzbso6qujzJOcBVdG/3v0XbryV/OskTgAeB36+qOwe1o43mMDRJ2tQ4BCFJjRjAktSIASxJjRjAktSIASxJjRjAktSIAayBSLLnmlOHJjkhyR+3qmlUHcf1x8COLH9h5Mssa7n98/upCa9OMrs/ZnUy+31jkt1HLZ+c5KmT2ZY2DQawNhr9TFlT4Ti6GbcAqKqXjTMz2euAD1bVflV1W1W9ZpL7fSPdXAcj+31ziylMNTwMYE27JH+Y5DtJrk3yL33btv2k+1f0s4Ud0re/McnZST4HjDlDVpJZSb7S91CvS/L8vv1jSRaPnqg+yR/SheAlSS7p225Jsktfw+fTTW5/XZLDk7wZOAx4b5LTR/fs+1noPphuwvdrk7y1b39vfz+uS7IwndfQzZF8el/nNkkuTTKvX+fIfjvXJXn/qPt2X5L/29f0jSS7DeBPolaqypOnKT8BewLXrdF2AvDHdHO9PrZv27E//0vgqJE24D+Bbel6jUuBndexr3cAf9Zf3hLYvr+886i2S4Fn9su3ALuMWv8WYBfgd4GPj2p/fH9+KvCaNe8X8Ba6CZlmrLG/nUdt41PAK/vLl9JNDs/oZboXhFuBmXRfmf4S3QT/0E2nObL+B4D3tP7bepq6kz1gDcravuNewLV0PcGj+OVUfy8Bju/n1rgU2BqY2193UVWtazL+K4A3JTkBeEZ1k+QDHJbkKrq5BZ4GjDfe+m3goCTvT/L8qrp7nNsfBPxjdRN3M6rGFyW5PMm3gd/u970uzwEurW42sNXA6XSTt0P3qwwjc3tcSfcCoE2EAaxBuQPYaY22nYHb6X4x46PAs4Er+7HdAL9b3TjrflU1t7q5WaGbO3etqvu1lBfQ/azOp5K8IcledL3t+VX1TODzdKG+ru38Z1/Tt4H/l+S949zHsMYLTbrfMvsHuh7zM4CPj7fffjtr82BVjezjITaiCbQ0PgNYA1HdzyQtTzIfIMnOdPMUfw2YU1WX0P1yxo7AdsAXgbf20xGSZP+J7ivJE+kmgv843TShzwJ2oAvuu/tx05eOWuVeYPsxtrM7cH9VnUb3Cw3jzdd7IfA/Rj4c7O/jSNjenm7C/tEf2I25X7pfU/mtfhx6S7opIr88zr61CfDVVIP0BuCjSf66X34f3VjnJUkeT9fz+5uquivJX9D9nNG1fQjfAkz0Bz1fCLwzyYN0P9z6hqr6QZJvAdfTTf34H6NuvxD4tyTLq+pFo9qfAfxVkofppiJ8yzj7PRn41b7mB+nGj/8+ycfpetG30A2PjDgV+MckPwV+8YvIVbU8ybuAS+geky9U1XkTvO/aiDkdpSQ14hCEJDXiEIQ2GkmeQXdY12g/r6pfb1GPtKEcgpCkRhyCkKRGDGBJasQAlqRGDGBJauT/A1n8ssDKmeEjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data, x=\"User_satisfication\").set(title=\"User Satisfication Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"category_name\"] = data[\"category_name\"].replace({\"'Bulldog'\": 1,\n",
    "                                                      \"'Maltese'\": 2,\n",
    "                                                      \"'Poodle'\": 3,\n",
    "                                                      \"'German Shepherd'\": 4,\n",
    "                                                      \"'Golden Retriever'\": 5,\n",
    "                                                      \"'Shi-ba-inu'\": 6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Age\", \"Gender\", \"Rehabilitation\", \"Pet_intimacy\", \"log_count\", \"com_count\", \"avg_loading\", \"category_name\", \"sum_duration\"]].values\n",
    "Y = data[[\"User_satisfication\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15181218, 0.        , 0.74008437, ..., 0.38998629, 0.14801687,\n",
       "        0.19892526],\n",
       "       [0.        , 0.        , 0.        , ..., 0.77631354, 0.62105083,\n",
       "        0.00277806],\n",
       "       [0.39305406, 0.        , 0.58958109, ..., 0.310484  , 0.        ,\n",
       "        0.13291947],\n",
       "       ...,\n",
       "       [0.55780589, 0.        , 0.7017558 , ..., 0.15106637, 0.28070232,\n",
       "        0.00306931],\n",
       "       [0.11284374, 0.51775365, 0.51775365, ..., 0.22154939, 0.41420292,\n",
       "        0.39217523],\n",
       "       [0.11525639, 0.        , 0.69153831, ..., 0.26162144, 0.13830766,\n",
       "        0.20044987]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "X_normal = preprocessing.normalize(X_scale)\n",
    "X_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_normal, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(X_test_val, Y_test_val, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  402 \n",
      "Validation size:  86 \n",
      "Test size:  87\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: \", X_train.shape[0], \"\\nValidation size: \", X_val.shape[0], \"\\nTest size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 23.3684 - mean_squared_error: 23.3684 - val_loss: 26.3258 - val_mean_squared_error: 26.3258\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22.0602 - mean_squared_error: 22.0602 - val_loss: 24.8660 - val_mean_squared_error: 24.8660\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 20.7966 - mean_squared_error: 20.7966 - val_loss: 23.4317 - val_mean_squared_error: 23.4317\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 19.5268 - mean_squared_error: 19.5268 - val_loss: 21.9873 - val_mean_squared_error: 21.9873\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 18.2111 - mean_squared_error: 18.2111 - val_loss: 20.4861 - val_mean_squared_error: 20.4861\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.8195 - mean_squared_error: 16.8195 - val_loss: 18.8396 - val_mean_squared_error: 18.8396\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.2698 - mean_squared_error: 15.2698 - val_loss: 17.0150 - val_mean_squared_error: 17.0150\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.5585 - mean_squared_error: 13.5585 - val_loss: 14.9814 - val_mean_squared_error: 14.9814\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6601 - mean_squared_error: 11.6601 - val_loss: 12.7328 - val_mean_squared_error: 12.7328\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6364 - mean_squared_error: 9.6364 - val_loss: 10.2944 - val_mean_squared_error: 10.2944\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5435 - mean_squared_error: 7.5435 - val_loss: 7.8530 - val_mean_squared_error: 7.8530\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5483 - mean_squared_error: 5.5483 - val_loss: 5.6081 - val_mean_squared_error: 5.6081\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8604 - mean_squared_error: 3.8604 - val_loss: 3.7754 - val_mean_squared_error: 3.7754\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7010 - mean_squared_error: 2.7010 - val_loss: 2.5096 - val_mean_squared_error: 2.5096\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0436 - mean_squared_error: 2.0436 - val_loss: 1.8451 - val_mean_squared_error: 1.8451\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7959 - mean_squared_error: 1.7959 - val_loss: 1.5797 - val_mean_squared_error: 1.5797\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7244 - mean_squared_error: 1.7244 - val_loss: 1.4671 - val_mean_squared_error: 1.4671\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6594 - mean_squared_error: 1.6594 - val_loss: 1.3835 - val_mean_squared_error: 1.3835\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5460 - mean_squared_error: 1.5460 - val_loss: 1.3200 - val_mean_squared_error: 1.3200\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4352 - mean_squared_error: 1.4352 - val_loss: 1.2722 - val_mean_squared_error: 1.2722\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3384 - mean_squared_error: 1.3384 - val_loss: 1.2218 - val_mean_squared_error: 1.2218\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2526 - mean_squared_error: 1.2526 - val_loss: 1.1599 - val_mean_squared_error: 1.1599\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1802 - mean_squared_error: 1.1802 - val_loss: 1.1025 - val_mean_squared_error: 1.1025\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1079 - mean_squared_error: 1.1079 - val_loss: 1.0334 - val_mean_squared_error: 1.0334\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0437 - mean_squared_error: 1.0437 - val_loss: 0.9614 - val_mean_squared_error: 0.9614\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9833 - mean_squared_error: 0.9833 - val_loss: 0.9018 - val_mean_squared_error: 0.9018\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9277 - mean_squared_error: 0.9277 - val_loss: 0.8554 - val_mean_squared_error: 0.8554\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8788 - mean_squared_error: 0.8788 - val_loss: 0.8178 - val_mean_squared_error: 0.8178\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8334 - mean_squared_error: 0.8334 - val_loss: 0.7808 - val_mean_squared_error: 0.7808\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7889 - mean_squared_error: 0.7889 - val_loss: 0.7459 - val_mean_squared_error: 0.7459\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7499 - mean_squared_error: 0.7499 - val_loss: 0.7142 - val_mean_squared_error: 0.7142\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7133 - mean_squared_error: 0.7133 - val_loss: 0.6843 - val_mean_squared_error: 0.6843\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6773 - mean_squared_error: 0.6773 - val_loss: 0.6579 - val_mean_squared_error: 0.6579\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6455 - mean_squared_error: 0.6455 - val_loss: 0.6325 - val_mean_squared_error: 0.6325\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6108 - mean_squared_error: 0.6108 - val_loss: 0.6073 - val_mean_squared_error: 0.6073\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5774 - mean_squared_error: 0.5774 - val_loss: 0.5830 - val_mean_squared_error: 0.5830\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5471 - mean_squared_error: 0.5471 - val_loss: 0.5607 - val_mean_squared_error: 0.5607\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5205 - mean_squared_error: 0.5205 - val_loss: 0.5422 - val_mean_squared_error: 0.5422\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4964 - mean_squared_error: 0.4964 - val_loss: 0.5264 - val_mean_squared_error: 0.5264\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4752 - mean_squared_error: 0.4752 - val_loss: 0.5107 - val_mean_squared_error: 0.5107\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4544 - mean_squared_error: 0.4544 - val_loss: 0.4958 - val_mean_squared_error: 0.4958\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4351 - mean_squared_error: 0.4351 - val_loss: 0.4816 - val_mean_squared_error: 0.4816\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4165 - mean_squared_error: 0.4165 - val_loss: 0.4668 - val_mean_squared_error: 0.4668\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3992 - mean_squared_error: 0.3992 - val_loss: 0.4533 - val_mean_squared_error: 0.4533\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3823 - mean_squared_error: 0.3823 - val_loss: 0.4393 - val_mean_squared_error: 0.4393\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.4236 - val_mean_squared_error: 0.4236\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3515 - mean_squared_error: 0.3515 - val_loss: 0.4085 - val_mean_squared_error: 0.4085\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3365 - mean_squared_error: 0.3365 - val_loss: 0.3942 - val_mean_squared_error: 0.3942\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3229 - mean_squared_error: 0.3229 - val_loss: 0.3800 - val_mean_squared_error: 0.3800\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3114 - mean_squared_error: 0.3114 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.2985 - mean_squared_error: 0.2985 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2872 - mean_squared_error: 0.2872 - val_loss: 0.3440 - val_mean_squared_error: 0.3440\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2769 - mean_squared_error: 0.2769 - val_loss: 0.3332 - val_mean_squared_error: 0.3332\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2669 - mean_squared_error: 0.2669 - val_loss: 0.3223 - val_mean_squared_error: 0.3223\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2577 - mean_squared_error: 0.2577 - val_loss: 0.3125 - val_mean_squared_error: 0.3125\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2488 - mean_squared_error: 0.2488 - val_loss: 0.3029 - val_mean_squared_error: 0.3029\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2408 - mean_squared_error: 0.2408 - val_loss: 0.2935 - val_mean_squared_error: 0.2935\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2330 - mean_squared_error: 0.2330 - val_loss: 0.2858 - val_mean_squared_error: 0.2858\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2254 - mean_squared_error: 0.2254 - val_loss: 0.2777 - val_mean_squared_error: 0.2777\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2188 - mean_squared_error: 0.2188 - val_loss: 0.2704 - val_mean_squared_error: 0.2704\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2121 - mean_squared_error: 0.2121 - val_loss: 0.2632 - val_mean_squared_error: 0.2632\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2062 - mean_squared_error: 0.2062 - val_loss: 0.2563 - val_mean_squared_error: 0.2563\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2001 - mean_squared_error: 0.2001 - val_loss: 0.2495 - val_mean_squared_error: 0.2495\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1945 - mean_squared_error: 0.1945 - val_loss: 0.2439 - val_mean_squared_error: 0.2439\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1895 - mean_squared_error: 0.1895 - val_loss: 0.2387 - val_mean_squared_error: 0.2387\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1848 - mean_squared_error: 0.1848 - val_loss: 0.2336 - val_mean_squared_error: 0.2336\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1801 - mean_squared_error: 0.1801 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1756 - mean_squared_error: 0.1756 - val_loss: 0.2227 - val_mean_squared_error: 0.2227\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1715 - mean_squared_error: 0.1715 - val_loss: 0.2173 - val_mean_squared_error: 0.2173\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1675 - mean_squared_error: 0.1675 - val_loss: 0.2130 - val_mean_squared_error: 0.2130\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1641 - mean_squared_error: 0.1641 - val_loss: 0.2085 - val_mean_squared_error: 0.2085\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1610 - mean_squared_error: 0.1610 - val_loss: 0.2040 - val_mean_squared_error: 0.2040\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1584 - mean_squared_error: 0.1584 - val_loss: 0.1989 - val_mean_squared_error: 0.1989\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1551 - mean_squared_error: 0.1551 - val_loss: 0.1954 - val_mean_squared_error: 0.1954\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1525 - mean_squared_error: 0.1525 - val_loss: 0.1923 - val_mean_squared_error: 0.1923\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1898 - val_mean_squared_error: 0.1898\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1480 - mean_squared_error: 0.1480 - val_loss: 0.1864 - val_mean_squared_error: 0.1864\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1462 - mean_squared_error: 0.1462 - val_loss: 0.1840 - val_mean_squared_error: 0.1840\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1437 - mean_squared_error: 0.1437 - val_loss: 0.1818 - val_mean_squared_error: 0.1818\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1419 - mean_squared_error: 0.1419 - val_loss: 0.1791 - val_mean_squared_error: 0.1791\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1401 - mean_squared_error: 0.1401 - val_loss: 0.1766 - val_mean_squared_error: 0.1766\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1381 - mean_squared_error: 0.1381 - val_loss: 0.1742 - val_mean_squared_error: 0.1742\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1366 - mean_squared_error: 0.1366 - val_loss: 0.1718 - val_mean_squared_error: 0.1718\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1353 - mean_squared_error: 0.1353 - val_loss: 0.1697 - val_mean_squared_error: 0.1697\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1335 - mean_squared_error: 0.1335 - val_loss: 0.1671 - val_mean_squared_error: 0.1671\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1318 - mean_squared_error: 0.1318 - val_loss: 0.1656 - val_mean_squared_error: 0.1656\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1299 - mean_squared_error: 0.1299 - val_loss: 0.1626 - val_mean_squared_error: 0.1626\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1283 - mean_squared_error: 0.1283 - val_loss: 0.1599 - val_mean_squared_error: 0.1599\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1276 - mean_squared_error: 0.1276 - val_loss: 0.1587 - val_mean_squared_error: 0.1587\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1247 - mean_squared_error: 0.1247 - val_loss: 0.1557 - val_mean_squared_error: 0.1557\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1241 - mean_squared_error: 0.1241 - val_loss: 0.1537 - val_mean_squared_error: 0.1537\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1220 - mean_squared_error: 0.1220 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1212 - mean_squared_error: 0.1212 - val_loss: 0.1502 - val_mean_squared_error: 0.1502\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1194 - mean_squared_error: 0.1194 - val_loss: 0.1475 - val_mean_squared_error: 0.1475\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1180 - mean_squared_error: 0.1180 - val_loss: 0.1458 - val_mean_squared_error: 0.1458\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1173 - mean_squared_error: 0.1173 - val_loss: 0.1445 - val_mean_squared_error: 0.1445\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1155 - mean_squared_error: 0.1155 - val_loss: 0.1422 - val_mean_squared_error: 0.1422\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1144 - mean_squared_error: 0.1144 - val_loss: 0.1414 - val_mean_squared_error: 0.1414\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1133 - mean_squared_error: 0.1133 - val_loss: 0.1405 - val_mean_squared_error: 0.1405\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1124 - mean_squared_error: 0.1124 - val_loss: 0.1381 - val_mean_squared_error: 0.1381\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1117 - mean_squared_error: 0.1117 - val_loss: 0.1372 - val_mean_squared_error: 0.1372\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1100 - mean_squared_error: 0.1100 - val_loss: 0.1344 - val_mean_squared_error: 0.1344\n",
      "Epoch 103/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1093 - mean_squared_error: 0.1093 - val_loss: 0.1332 - val_mean_squared_error: 0.1332\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1078 - mean_squared_error: 0.1078 - val_loss: 0.1339 - val_mean_squared_error: 0.1339\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1073 - mean_squared_error: 0.1073 - val_loss: 0.1323 - val_mean_squared_error: 0.1323\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1064 - mean_squared_error: 0.1064 - val_loss: 0.1311 - val_mean_squared_error: 0.1311\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1056 - mean_squared_error: 0.1056 - val_loss: 0.1302 - val_mean_squared_error: 0.1302\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1049 - mean_squared_error: 0.1049 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1041 - mean_squared_error: 0.1041 - val_loss: 0.1281 - val_mean_squared_error: 0.1281\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1038 - mean_squared_error: 0.1038 - val_loss: 0.1275 - val_mean_squared_error: 0.1275\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1022 - mean_squared_error: 0.1022 - val_loss: 0.1287 - val_mean_squared_error: 0.1287\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1025 - mean_squared_error: 0.1025 - val_loss: 0.1290 - val_mean_squared_error: 0.1290\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1012 - mean_squared_error: 0.1012 - val_loss: 0.1253 - val_mean_squared_error: 0.1253\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1012 - mean_squared_error: 0.1012 - val_loss: 0.1257 - val_mean_squared_error: 0.1257\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.1000 - mean_squared_error: 0.1000 - val_loss: 0.1287 - val_mean_squared_error: 0.1287\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0993 - mean_squared_error: 0.0993 - val_loss: 0.1248 - val_mean_squared_error: 0.1248\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0989 - mean_squared_error: 0.0989 - val_loss: 0.1241 - val_mean_squared_error: 0.1241\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0980 - mean_squared_error: 0.0980 - val_loss: 0.1246 - val_mean_squared_error: 0.1246\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0979 - mean_squared_error: 0.0979 - val_loss: 0.1255 - val_mean_squared_error: 0.1255\n",
      "Epoch 120/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0973 - mean_squared_error: 0.0973 - val_loss: 0.1230 - val_mean_squared_error: 0.1230\n",
      "Epoch 121/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0964 - mean_squared_error: 0.0964 - val_loss: 0.1241 - val_mean_squared_error: 0.1241\n",
      "Epoch 122/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0962 - mean_squared_error: 0.0962 - val_loss: 0.1238 - val_mean_squared_error: 0.1238\n",
      "Epoch 123/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0955 - mean_squared_error: 0.0955 - val_loss: 0.1219 - val_mean_squared_error: 0.1219\n",
      "Epoch 124/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0951 - mean_squared_error: 0.0951 - val_loss: 0.1235 - val_mean_squared_error: 0.1235\n",
      "Epoch 125/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0955 - mean_squared_error: 0.0955 - val_loss: 0.1238 - val_mean_squared_error: 0.1238\n",
      "Epoch 126/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0944 - mean_squared_error: 0.0944 - val_loss: 0.1213 - val_mean_squared_error: 0.1213\n",
      "Epoch 127/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0939 - mean_squared_error: 0.0939 - val_loss: 0.1221 - val_mean_squared_error: 0.1221\n",
      "Epoch 128/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0932 - mean_squared_error: 0.0932 - val_loss: 0.1225 - val_mean_squared_error: 0.1225\n",
      "Epoch 129/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0932 - mean_squared_error: 0.0932 - val_loss: 0.1222 - val_mean_squared_error: 0.1222\n",
      "Epoch 130/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0927 - mean_squared_error: 0.0927 - val_loss: 0.1210 - val_mean_squared_error: 0.1210\n",
      "Epoch 131/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0924 - mean_squared_error: 0.0924 - val_loss: 0.1219 - val_mean_squared_error: 0.1219\n",
      "Epoch 132/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0919 - mean_squared_error: 0.0919 - val_loss: 0.1215 - val_mean_squared_error: 0.1215\n",
      "Epoch 133/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0914 - mean_squared_error: 0.0914 - val_loss: 0.1207 - val_mean_squared_error: 0.1207\n",
      "Epoch 134/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0911 - mean_squared_error: 0.0911 - val_loss: 0.1220 - val_mean_squared_error: 0.1220\n",
      "Epoch 135/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0910 - mean_squared_error: 0.0910 - val_loss: 0.1219 - val_mean_squared_error: 0.1219\n",
      "Epoch 136/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0905 - mean_squared_error: 0.0905 - val_loss: 0.1212 - val_mean_squared_error: 0.1212\n",
      "Epoch 137/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.1203 - val_mean_squared_error: 0.1203\n",
      "Epoch 138/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.1224 - val_mean_squared_error: 0.1224\n",
      "Epoch 139/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0904 - mean_squared_error: 0.0904 - val_loss: 0.1200 - val_mean_squared_error: 0.1200\n",
      "Epoch 140/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0898 - mean_squared_error: 0.0898 - val_loss: 0.1224 - val_mean_squared_error: 0.1224\n",
      "Epoch 141/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0898 - mean_squared_error: 0.0898 - val_loss: 0.1206 - val_mean_squared_error: 0.1206\n",
      "Epoch 142/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0892 - mean_squared_error: 0.0892 - val_loss: 0.1194 - val_mean_squared_error: 0.1194\n",
      "Epoch 143/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0890 - mean_squared_error: 0.0890 - val_loss: 0.1228 - val_mean_squared_error: 0.1228\n",
      "Epoch 144/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.1217 - val_mean_squared_error: 0.1217\n",
      "Epoch 145/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0878 - mean_squared_error: 0.0878 - val_loss: 0.1198 - val_mean_squared_error: 0.1198\n",
      "Epoch 146/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0885 - mean_squared_error: 0.0885 - val_loss: 0.1217 - val_mean_squared_error: 0.1217\n",
      "Epoch 147/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0884 - mean_squared_error: 0.0884 - val_loss: 0.1231 - val_mean_squared_error: 0.1231\n",
      "Epoch 148/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.1207 - val_mean_squared_error: 0.1207\n",
      "Epoch 149/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.1215 - val_mean_squared_error: 0.1215\n",
      "Epoch 150/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.1229 - val_mean_squared_error: 0.1229\n"
     ]
    }
   ],
   "source": [
    "# There are 9 columns in the X data, meaning we are using 8 features to make predictions\n",
    "# We would build a neural network with two hidden layers of size 32 and output a single predicted value of y(the satisfication level)\n",
    "# website to draw nn archetecture: http://alexlenail.me/NN-SVG/index.html\n",
    "nn_model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(9,)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='adam',\n",
    "                loss=\"mse\",\n",
    "                metrics=['mean_squared_error'])\n",
    "Y_pred = nn_model.fit(X_train, Y_train, batch_size=70, epochs=150, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1356 - mean_squared_error: 0.1356\n"
     ]
    }
   ],
   "source": [
    "score = nn_model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# == Above are the code for Neural Network, below were previously wrote =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class NN():\n",
    "    def __init__(self, input_n, layers=[256,128], output_n, lr, iterations):\n",
    "        self.input_layer = input_n\n",
    "        self.output_layer = output_n\n",
    "        self.fc1 = nn.Linear(self.input_layer, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], self.output_layer)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.iterations = iterations\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "class NN_train_test():\n",
    "    def __init__(self, input_n, layers, output_n, lr， iterations, num_epochs):\n",
    "        self.model = NN(input_n=input_n, layers=layers, output_n=output_n, lr=lr, iterations=iterations)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=lr)\n",
    "        self.num_epochs = num_epochs\n",
    "    def train(X, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            correct = 0\n",
    "            l = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                data = X.iloc[i, :]\n",
    "                result = y.iloc[i, :]\n",
    "                prediction = self.model(data)\n",
    "                loss = self.criterion(prediction, result)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                _, predictions = prediction.max(dim=1)\n",
    "                correct += torch.eq(predictions, result).sum().item()\n",
    "                l += result.shape[0]\n",
    "            accuracy = correct / l\n",
    "            print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "\n",
    "    def test(X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpKEc4OTYxRo"
   },
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "input = data.loc[:, 'Have diseases':\"Recovery\"]\n",
    "target = data[[\"Recovery\"]]\n",
    "\n",
    "X, y = input, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieSUPdkMRXd9"
   },
   "outputs": [],
   "source": [
    "iterations_n = 1000\n",
    "learning_rate = 0.01\n",
    "num_feature = len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yps8b_Gqtx17"
   },
   "outputs": [],
   "source": [
    "class PytorchLRModel(nn.Module):\n",
    "    def __init__(self, num_feature):\n",
    "        super(PytorchLRModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_feature, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x[:,1]\n",
    "\n",
    "clf = PytorchLRModel(num_feature)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Y05PFe8qXEE",
    "outputId": "d664d87b-84a0-4a29-bd8d-da72202086be"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(clf.parameters(), lr=learning_rate)\n",
    "input = Variable(torch.Tensor(X_train.values))\n",
    "target = Variable(torch.Tensor(y_train.values)).long()\n",
    "\n",
    "clf.train()\n",
    "for iter_ in range(iterations_n):\n",
    "  optimizer.zero_grad()\n",
    "  scores = clf(input)\n",
    "  loss = criterion(scores.unsqueeze(1), target.float())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if iter_ % 100 == 0:\n",
    "    print('Iteration {}, Loss: {}'.format(iter_, loss.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfgVPnT3vp5a",
    "outputId": "366dbeb4-8f45-445e-b22b-a5fb348a8db5"
   },
   "outputs": [],
   "source": [
    "clf.eval()\n",
    "target = Variable(torch.Tensor(y_test.values)).long()\n",
    "y_hat = clf(Variable(torch.Tensor(X_test.values)))\n",
    "predictions = (y_hat >= 0.5).float()\n",
    "print(predictions)\n",
    "print(target.shape)\n",
    "accuracy = torch.eq(predictions, target).sum().item() / target.size()[0]\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_RVwIxOyeZiT",
    "outputId": "d09c43ca-3f2a-44f5-bb8b-af7c404d6aa3"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators = 500, max_features = 'sqrt', max_depth = 8, random_state = 18).fit(X_train, y_train)\n",
    "prediction = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "rmse = mse**.5\n",
    "print(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztcW4YAJedP3"
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class NN():\n",
    "    def __init__(self, input_n, layers=[256,128], output_n, lr, iterations):\n",
    "        self.input_layer = input_n\n",
    "        self.output_layer = output_n\n",
    "        self.fc1 = nn.Linear(self.input_layer, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], self.output_layer)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.iterations = iterations\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "class NN_train_test():\n",
    "    def __init__(self, input_n, layers, output_n, lr， iterations, num_epochs):\n",
    "        self.model = NN(input_n=input_n, layers=layers, output_n=output_n, lr=lr, iterations=iterations)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=lr)\n",
    "        self.num_epochs = num_epochs\n",
    "    def train(X, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            correct = 0\n",
    "            l = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                data = X.iloc[i, :]\n",
    "                result = y.iloc[i, :]\n",
    "                prediction = self.model(data)\n",
    "                loss = self.criterion(prediction, result)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                _, predictions = prediction.max(dim=1)\n",
    "                correct += torch.eq(predictions, result).sum().item()\n",
    "                l += result.shape[0]\n",
    "            accuracy = correct / l\n",
    "            print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "\n",
    "    def test(X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWKsGGTtZwgh"
   },
   "outputs": [],
   "source": [
    "# RNN\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
